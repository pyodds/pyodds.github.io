

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>algo package &mdash; pyodds 1.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="utils package" href="utils.html" />
    <link rel="prev" title="pyodds" href="modules.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> pyodds
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Documentation:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="introduction.html">PyODDS</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="apicheatsheet.html">APIs Cheatsheet</a></li>
<li class="toctree-l1"><a class="reference internal" href="implementedalgos.html">Implemented Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="license.html">License</a></li>
<li class="toctree-l1"><a class="reference internal" href="license.html#cite-this-work">Cite this work</a></li>
</ul>
<p class="caption"><span class="caption-text">Package Reference:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">pyodds</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">algo package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#submodules">Submodules</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-algo.algorithm_utils">algo.algorithm_utils module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-algo.autoencoder">algo.autoencoder module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-algo.base">algo.base module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-algo.cblof">algo.cblof module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-algo.dagmm">algo.dagmm module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-algo.hbos">algo.hbos module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-algo.iforest">algo.iforest module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-algo.knn">algo.knn module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-algo.lof">algo.lof module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-algo.lstmad">algo.lstmad module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-algo.lstmencdec">algo.lstmencdec module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-algo.luminolFunc">algo.luminolFunc module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-algo.ocsvm">algo.ocsvm module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-algo.pca">algo.pca module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-algo.robustcovariance">algo.robustcovariance module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-algo.sod">algo.sod module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-algo.staticautoencoder">algo.staticautoencoder module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-algo">Module contents</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="utils.html">utils package</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">pyodds</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
          <li><a href="modules.html">pyodds</a> &raquo;</li>
        
      <li>algo package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="https://github.com/datamllab/PyODDS" rel="nofollow"> View Github Page</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="algo-package">
<h1>algo package<a class="headerlink" href="#algo-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-algo.algorithm_utils">
<span id="algo-algorithm-utils-module"></span><h2>algo.algorithm_utils module<a class="headerlink" href="#module-algo.algorithm_utils" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="algo.algorithm_utils.PyTorchUtils">
<em class="property">class </em><code class="sig-prename descclassname">algo.algorithm_utils.</code><code class="sig-name descname">PyTorchUtils</code><span class="sig-paren">(</span><em class="sig-param">seed</em>, <em class="sig-param">gpu</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.algorithm_utils.PyTorchUtils" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Abstract class for PyTorch based deep learning  detection algorithms.</p>
<dl class="method">
<dt id="algo.algorithm_utils.PyTorchUtils.device">
<em class="property">property </em><code class="sig-name descname">device</code><a class="headerlink" href="#algo.algorithm_utils.PyTorchUtils.device" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="algo.algorithm_utils.PyTorchUtils.to_device">
<code class="sig-name descname">to_device</code><span class="sig-paren">(</span><em class="sig-param">model</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.algorithm_utils.PyTorchUtils.to_device" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="algo.algorithm_utils.PyTorchUtils.to_var">
<code class="sig-name descname">to_var</code><span class="sig-paren">(</span><em class="sig-param">t</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.algorithm_utils.PyTorchUtils.to_var" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="algo.algorithm_utils.TensorflowUtils">
<em class="property">class </em><code class="sig-prename descclassname">algo.algorithm_utils.</code><code class="sig-name descname">TensorflowUtils</code><span class="sig-paren">(</span><em class="sig-param">seed</em>, <em class="sig-param">gpu</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.algorithm_utils.TensorflowUtils" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Abstract class for Tensorflow based deep learning  detection algorithms.</p>
<dl class="method">
<dt id="algo.algorithm_utils.TensorflowUtils.device">
<em class="property">property </em><code class="sig-name descname">device</code><a class="headerlink" href="#algo.algorithm_utils.TensorflowUtils.device" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="algo.algorithm_utils.deepBase">
<em class="property">class </em><code class="sig-prename descclassname">algo.algorithm_utils.</code><code class="sig-name descname">deepBase</code><span class="sig-paren">(</span><em class="sig-param">module_name</em>, <em class="sig-param">name</em>, <em class="sig-param">seed</em>, <em class="sig-param">details=False</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.algorithm_utils.deepBase" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Abstract class for deep learning based detection algorithms.</p>
<dl class="method">
<dt id="algo.algorithm_utils.deepBase.fit">
<em class="property">abstract </em><code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.algorithm_utils.deepBase.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Train the algorithm on the given dataset</p>
</dd></dl>

<dl class="method">
<dt id="algo.algorithm_utils.deepBase.predict">
<em class="property">abstract </em><code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.algorithm_utils.deepBase.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>:return anomaly score</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-algo.autoencoder">
<span id="algo-autoencoder-module"></span><h2>algo.autoencoder module<a class="headerlink" href="#module-algo.autoencoder" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="algo.autoencoder.AUTOENCODER">
<em class="property">class </em><code class="sig-prename descclassname">algo.autoencoder.</code><code class="sig-name descname">AUTOENCODER</code><span class="sig-paren">(</span><em class="sig-param">name: str = 'AutoEncoder'</em>, <em class="sig-param">num_epochs: int = 10</em>, <em class="sig-param">batch_size: int = 20</em>, <em class="sig-param">lr: float = 0.001</em>, <em class="sig-param">hidden_size: int = 5</em>, <em class="sig-param">sequence_length: int = 30</em>, <em class="sig-param">train_gaussian_percentage: float = 0.25</em>, <em class="sig-param">seed: int = None</em>, <em class="sig-param">gpu: int = None</em>, <em class="sig-param">details=True</em>, <em class="sig-param">contamination=0.05</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.autoencoder.AUTOENCODER" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">pyodds.algo.base.Base</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">pyodds.algo.algorithm_utils.deepBase</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">pyodds.algo.algorithm_utils.PyTorchUtils</span></code></p>
<p>Auto Encoder (AE) is a type of neural networks for learning useful data representations unsupervisedly.
It could be used to detect outlying objects in the data by calculating the reconstruction errors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>str</em><em>, </em><em>optional</em><em> (</em><em>default='AutoEncoder'</em><em>)</em>) – The name of the algorithm</p></li>
<li><p><strong>num_epochs</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default=10</em><em>)</em>) – The number of epochs</p></li>
<li><p><strong>batch_size</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default=20</em><em>)</em>) – The number of batch size</p></li>
<li><p><strong>lr</strong> (<em>float</em><em>, </em><em>optional</em><em> (</em><em>default=1e-3</em><em>)</em>) – The speed of learning rate</p></li>
<li><p><strong>hidden_size</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default=5</em><em>)</em>) – The number of hidden layer</p></li>
<li><p><strong>sequence_length</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default=30</em><em>)</em>) – The length of sequence</p></li>
<li><p><strong>train_gaussian_percentage</strong> (<em>float</em><em>, </em><em>optional</em><em> (</em><em>default=0.25</em><em>)</em>) – The percentage for gaussian training</p></li>
<li><p><strong>seed</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default=None</em><em>)</em>) – The random seed</p></li>
<li><p><strong>contamination</strong> (<em>float in</em><em> (</em><em>0.</em><em>, </em><em>0.5</em><em>)</em><em>, </em><em>optional</em><em> (</em><em>default=0.05</em><em>)</em>) – The percentage of outliers</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="algo.autoencoder.AUTOENCODER.decision_function">
<code class="sig-name descname">decision_function</code><span class="sig-paren">(</span><em class="sig-param">X: pandas.core.frame.DataFrame</em><span class="sig-paren">)</span> &#x2192; numpy.array<a class="headerlink" href="#algo.autoencoder.AUTOENCODER.decision_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.
:param X: The training input samples. Sparse matrices are accepted only</p>
<blockquote>
<div><p>if they are supported by the base estimator.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>anomaly_scores</strong> – The anomaly score of the input samples.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>numpy array of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="algo.autoencoder.AUTOENCODER.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">X: pandas.core.frame.DataFrame</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.autoencoder.AUTOENCODER.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit detector.
:param X: The input samples.
:type X: dataframe of shape (n_samples, n_features)</p>
</dd></dl>

<dl class="method">
<dt id="algo.autoencoder.AUTOENCODER.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.autoencoder.AUTOENCODER.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Return outliers with -1 and inliers with 1, with the outlierness score calculated from the <cite>decision_function(X)</cite>,
and the threshold <cite>contamination</cite>.
:param X: The input samples.
:type X: dataframe of shape (n_samples, n_features)</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>ranking</strong> – The outlierness of the input samples.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>numpy array of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="algo.autoencoder.AutoEncoderModule">
<em class="property">class </em><code class="sig-prename descclassname">algo.autoencoder.</code><code class="sig-name descname">AutoEncoderModule</code><span class="sig-paren">(</span><em class="sig-param">n_features: int</em>, <em class="sig-param">sequence_length: int</em>, <em class="sig-param">hidden_size: int</em>, <em class="sig-param">seed: int</em>, <em class="sig-param">gpu: int</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.autoencoder.AutoEncoderModule" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">pyodds.algo.algorithm_utils.PyTorchUtils</span></code></p>
<dl class="method">
<dt id="algo.autoencoder.AutoEncoderModule.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">ts_batch</em>, <em class="sig-param">return_latent: bool = False</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.autoencoder.AutoEncoderModule.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-algo.base">
<span id="algo-base-module"></span><h2>algo.base module<a class="headerlink" href="#module-algo.base" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="algo.base.Base">
<em class="property">class </em><code class="sig-prename descclassname">algo.base.</code><code class="sig-name descname">Base</code><a class="headerlink" href="#algo.base.Base" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Abstract class for all outlier detection algorithms.</p>
<dl class="method">
<dt id="algo.base.Base.decision_function">
<code class="sig-name descname">decision_function</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.base.Base.decision_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict raw anomaly scores of X using the fitted detector.
The anomaly score of an input sample is computed based on the fitted
detector. For consistency, outliers are assigned with
higher anomaly scores.
:param X: The input samples. Sparse matrices are accepted only</p>
<blockquote>
<div><p>if they are supported by the base estimator.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>anomaly_scores</strong> – The anomaly score of the input samples.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>numpy array of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="algo.base.Base.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.base.Base.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit detector.
:param X: The input samples.
:type X: numpy array of shape (n_samples, n_features)</p>
</dd></dl>

<dl class="method">
<dt id="algo.base.Base.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.base.Base.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Return outliers with -1 and inliers with 1, with the outlierness score calculated from the <a href="#id1"><span class="problematic" id="id2">`</span></a>decision_function(X)’,
and the threshold <a href="#id3"><span class="problematic" id="id4">`</span></a>contamination’.
:param X: The input samples.
:type X: dataframe of shape (n_samples, n_features)</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>ranking</strong> – The outlierness of the input samples.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>numpy array of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-algo.cblof">
<span id="algo-cblof-module"></span><h2>algo.cblof module<a class="headerlink" href="#module-algo.cblof" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="algo.cblof.CBLOF">
<em class="property">class </em><code class="sig-prename descclassname">algo.cblof.</code><code class="sig-name descname">CBLOF</code><span class="sig-paren">(</span><em class="sig-param">n_clusters=8</em>, <em class="sig-param">contamination=0.1</em>, <em class="sig-param">clustering_estimator=None</em>, <em class="sig-param">alpha=0.9</em>, <em class="sig-param">beta=5</em>, <em class="sig-param">use_weights=False</em>, <em class="sig-param">random_state=None</em>, <em class="sig-param">n_jobs=1</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.cblof.CBLOF" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">pyodds.algo.base.Base</span></code></p>
<p>The CBLOF operator calculates the outlier score based on cluster-based
local outlier factor.
CBLOF takes as an input the data set and the cluster model that was
generated by a clustering algorithm. It classifies the clusters into small
clusters and large clusters using the parameters alpha and beta.
The anomaly score is then calculated based on the size of the cluster the
point belongs to as well as the distance to the nearest large cluster.
Use weighting for outlier factor based on the sizes of the clusters as
proposed in the original publication. Since this might lead to unexpected
behavior (outliers close to small clusters are not found), it is disabled
by default.Outliers scores are solely computed based on their distance to
the closest large cluster center.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>n_clusters</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default=8</em><em>)</em>) – The number of clusters to form as well as the number of
centroids to generate.</p>
</dd>
</dl>
<dl class="simple">
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt><dd><p>The amount of contamination of the data set,
i.e. the proportion of outliers in the data set. Used when fitting to
define the threshold on the decision function.</p>
</dd>
<dt>clustering_estimator<span class="classifier">Estimator, optional (default=None)</span></dt><dd><p>The base clustering algorithm for performing data clustering.
A valid clustering algorithm should be passed in. The estimator should
have standard sklearn APIs, fit() and predict(). The estimator should
have attributes <code class="docutils literal notranslate"><span class="pre">labels_</span></code> and <code class="docutils literal notranslate"><span class="pre">cluster_centers_</span></code>.
If <code class="docutils literal notranslate"><span class="pre">cluster_centers_</span></code> is not in the attributes once the model is fit,
it is calculated as the mean of the samples in a cluster.
If not set, CBLOF uses KMeans for scalability. See
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html">https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html</a></p>
</dd>
<dt>alpha<span class="classifier">float in (0.5, 1), optional (default=0.9)</span></dt><dd><p>Coefficient for deciding small and large clusters. The ratio
of the number of samples in large clusters to the number of samples in
small clusters.</p>
</dd>
<dt>beta<span class="classifier">int or float in (1,), optional (default=5).</span></dt><dd><p>Coefficient for deciding small and large clusters. For a list
sorted clusters by size <cite>|C1|, |C2|, …, |Cn|, beta = |Ck|/|Ck-1|</cite></p>
</dd>
<dt>use_weights<span class="classifier">bool, optional (default=False)</span></dt><dd><p>If set to True, the size of clusters are used as weights in
outlier score calculation.</p>
</dd>
<dt>check_estimator<span class="classifier">bool, optional (default=False)</span></dt><dd><p>If set to True, check whether the base estimator is consistent with
sklearn standard.</p>
</dd>
</dl>
<dl class="method">
<dt id="algo.cblof.CBLOF.decision_function">
<code class="sig-name descname">decision_function</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.cblof.CBLOF.decision_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.
:param X: The training input samples. Sparse matrices are accepted only</p>
<blockquote>
<div><p>if they are supported by the base estimator.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>anomaly_scores</strong> – The anomaly score of the input samples.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>numpy array of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="algo.cblof.CBLOF.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.cblof.CBLOF.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit detector.
:param X: The input samples.
:type X: dataframe of shape (n_samples, n_features)</p>
</dd></dl>

<dl class="method">
<dt id="algo.cblof.CBLOF.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.cblof.CBLOF.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Return outliers with -1 and inliers with 1, with the outlierness score calculated from the <a href="#id5"><span class="problematic" id="id6">`</span></a>decision_function(X)’,
and the threshold <a href="#id7"><span class="problematic" id="id8">`</span></a>contamination’.
:param X: The input samples.
:type X: dataframe of shape (n_samples, n_features)</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>ranking</strong> – The outlierness of the input samples.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>numpy array of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="algo.cblof.pairwise_distances_no_broadcast">
<code class="sig-prename descclassname">algo.cblof.</code><code class="sig-name descname">pairwise_distances_no_broadcast</code><span class="sig-paren">(</span><em class="sig-param">X</em>, <em class="sig-param">Y</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.cblof.pairwise_distances_no_broadcast" title="Permalink to this definition">¶</a></dt>
<dd><p>Utility function to calculate row-wise euclidean distance of two matrix.
Different from pair-wise calculation, this function would not broadcast.
For instance, X and Y are both (4,3) matrices, the function would return
a distance vector with shape (4,), instead of (4,4).
:param X: First input samples
:type X: array of shape (n_samples, n_features)
:param Y: Second input samples
:type Y: array of shape (n_samples, n_features)</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>distance</strong> – Row-wise euclidean distance of X and Y</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>array of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-algo.dagmm">
<span id="algo-dagmm-module"></span><h2>algo.dagmm module<a class="headerlink" href="#module-algo.dagmm" title="Permalink to this headline">¶</a></h2>
<p>Adapted from Daniel Stanley Tan (<a class="reference external" href="https://github.com/danieltan07/dagmm">https://github.com/danieltan07/dagmm</a>)</p>
<dl class="class">
<dt id="algo.dagmm.DAGMM">
<em class="property">class </em><code class="sig-prename descclassname">algo.dagmm.</code><code class="sig-name descname">DAGMM</code><span class="sig-paren">(</span><em class="sig-param">num_epochs=10</em>, <em class="sig-param">lambda_energy=0.1</em>, <em class="sig-param">lambda_cov_diag=0.005</em>, <em class="sig-param">lr=0.001</em>, <em class="sig-param">batch_size=50</em>, <em class="sig-param">gmm_k=3</em>, <em class="sig-param">normal_percentile=80</em>, <em class="sig-param">sequence_length=30</em>, <em class="sig-param">autoencoder_type=&lt;class 'pyodds.algo.autoencoder.AutoEncoderModule'&gt;</em>, <em class="sig-param">autoencoder_args=None</em>, <em class="sig-param">hidden_size: int = 5</em>, <em class="sig-param">seed: int = None</em>, <em class="sig-param">gpu: int = None</em>, <em class="sig-param">details=True</em>, <em class="sig-param">contamination=0.05</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.dagmm.DAGMM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">pyodds.algo.base.Base</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">pyodds.algo.algorithm_utils.deepBase</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">pyodds.algo.algorithm_utils.PyTorchUtils</span></code></p>
<p>Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection, Zong et al, 2018.
Unsupervised anomaly detection on multi- or high-dimensional data is of great importance in both fundamental machine learning research and industrial applications, for which density estimation lies at the core. Although previous approaches based on dimensionality reduction followed by density estimation have made fruitful progress, they mainly suffer from decoupled model learning with inconsistent optimization goals and incapability of preserving essential information in the low-dimensional space. In this paper, we present a Deep Autoencoding Gaussian Mixture Model (DAGMM) for unsupervised anomaly detection. Our model utilizes a deep autoencoder to generate a low-dimensional representation and reconstruction error for each input data point, which is further fed into a Gaussian Mixture Model (GMM). Instead of using decoupled two-stage training and the standard Expectation-Maximization (EM) algorithm, DAGMM jointly optimizes the parameters of the deep autoencoder and the mixture model simultaneously in an end-to-end fashion, leveraging a separate estimation network to facilitate the parameter learning of the mixture model. The joint optimization, which well balances autoencoding reconstruction, density estimation of latent representation, and regularization, helps the autoencoder escape from less attractive local optima and further reduce reconstruction errors, avoiding the need of pre-training.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_epochs</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default=10</em><em>)</em>) – The number of epochs</p></li>
<li><p><strong>lambda_energy</strong> (<em>float</em><em>, </em><em>optional</em><em> (</em><em>default=0.1</em><em>)</em>) – The parameter to balance the energy in loss function</p></li>
<li><p><strong>lambda_cov_diag</strong> (<em>float</em><em>, </em><em>optional</em><em> (</em><em>default=0.05</em><em>)</em>) – The parameter to balance the covariance in loss function</p></li>
<li><p><strong>lr</strong> (<em>float</em><em>, </em><em>optional</em><em> (</em><em>default=1e-3</em><em>)</em>) – The speed of learning rate</p></li>
<li><p><strong>batch_size</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default=50</em><em>)</em>) – The number of samples in one batch</p></li>
<li><p><strong>gmm_k</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default=3</em><em>)</em>) – The number of clusters in the Gaussian Mixture model</p></li>
<li><p><strong>sequence_length</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default=30</em><em>)</em>) – The length of sequence</p></li>
<li><p><strong>hidden_size</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default=5</em><em>)</em>) – The size of hidden layer</p></li>
<li><p><strong>seed</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default=None</em><em>)</em>) – The random seed</p></li>
<li><p><strong>contamination</strong> (<em>float in</em><em> (</em><em>0.</em><em>, </em><em>0.5</em><em>)</em><em>, </em><em>optional</em><em> (</em><em>default=0.05</em><em>)</em>) – The percentage of outliers</p></li>
</ul>
</dd>
</dl>
<dl class="class">
<dt id="algo.dagmm.DAGMM.AutoEncoder">
<em class="property">class </em><code class="sig-name descname">AutoEncoder</code><a class="headerlink" href="#algo.dagmm.DAGMM.AutoEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="attribute">
<dt id="algo.dagmm.DAGMM.AutoEncoder.LSTM">
<code class="sig-name descname">LSTM</code><a class="headerlink" href="#algo.dagmm.DAGMM.AutoEncoder.LSTM" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">pyodds.algo.lstmencdec.LSTMEDModule</span></code></p>
</dd></dl>

<dl class="attribute">
<dt id="algo.dagmm.DAGMM.AutoEncoder.NN">
<code class="sig-name descname">NN</code><a class="headerlink" href="#algo.dagmm.DAGMM.AutoEncoder.NN" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">pyodds.algo.autoencoder.AutoEncoderModule</span></code></p>
</dd></dl>

</dd></dl>

<dl class="method">
<dt id="algo.dagmm.DAGMM.dagmm_step">
<code class="sig-name descname">dagmm_step</code><span class="sig-paren">(</span><em class="sig-param">input_data</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.dagmm.DAGMM.dagmm_step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="algo.dagmm.DAGMM.decision_function">
<code class="sig-name descname">decision_function</code><span class="sig-paren">(</span><em class="sig-param">X: pandas.core.frame.DataFrame</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.dagmm.DAGMM.decision_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.
Using the learned mixture probability, mean and covariance for each component k, compute the energy on the
given data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>dataframe of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>anomaly_scores</strong> – The anomaly score of the input samples.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>numpy array of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="algo.dagmm.DAGMM.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">X: pandas.core.frame.DataFrame</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.dagmm.DAGMM.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Learn the mixture probability, mean and covariance for each component k.
Store the computed energy based on the training data and the aforementioned parameters.
:param X: The input samples.
:type X: dataframe of shape (n_samples, n_features)</p>
</dd></dl>

<dl class="method">
<dt id="algo.dagmm.DAGMM.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.dagmm.DAGMM.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Return outliers with -1 and inliers with 1, with the outlierness score calculated from the <a href="#id9"><span class="problematic" id="id10">`</span></a>decision_function(X)’,
and the threshold <a href="#id11"><span class="problematic" id="id12">`</span></a>contamination’.
:param X: The input samples.
:type X: dataframe of shape (n_samples, n_features)</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>ranking</strong> – The outlierness of the input samples.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>numpy array of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="algo.dagmm.DAGMM.reset_grad">
<code class="sig-name descname">reset_grad</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#algo.dagmm.DAGMM.reset_grad" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="algo.dagmm.DAGMMModule">
<em class="property">class </em><code class="sig-prename descclassname">algo.dagmm.</code><code class="sig-name descname">DAGMMModule</code><span class="sig-paren">(</span><em class="sig-param">autoencoder</em>, <em class="sig-param">n_gmm</em>, <em class="sig-param">latent_dim</em>, <em class="sig-param">seed: int</em>, <em class="sig-param">gpu: int</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.dagmm.DAGMMModule" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">pyodds.algo.algorithm_utils.PyTorchUtils</span></code></p>
<p>Residual Block.</p>
<dl class="method">
<dt id="algo.dagmm.DAGMMModule.compute_energy">
<code class="sig-name descname">compute_energy</code><span class="sig-paren">(</span><em class="sig-param">z</em>, <em class="sig-param">phi=None</em>, <em class="sig-param">mu=None</em>, <em class="sig-param">cov=None</em>, <em class="sig-param">size_average=True</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.dagmm.DAGMMModule.compute_energy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="algo.dagmm.DAGMMModule.compute_gmm_params">
<code class="sig-name descname">compute_gmm_params</code><span class="sig-paren">(</span><em class="sig-param">z</em>, <em class="sig-param">gamma</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.dagmm.DAGMMModule.compute_gmm_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="algo.dagmm.DAGMMModule.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.dagmm.DAGMMModule.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="algo.dagmm.DAGMMModule.loss_function">
<code class="sig-name descname">loss_function</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">x_hat</em>, <em class="sig-param">z</em>, <em class="sig-param">gamma</em>, <em class="sig-param">lambda_energy</em>, <em class="sig-param">lambda_cov_diag</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.dagmm.DAGMMModule.loss_function" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="algo.dagmm.DAGMMModule.relative_euclidean_distance">
<code class="sig-name descname">relative_euclidean_distance</code><span class="sig-paren">(</span><em class="sig-param">a</em>, <em class="sig-param">b</em>, <em class="sig-param">dim=1</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.dagmm.DAGMMModule.relative_euclidean_distance" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-algo.hbos">
<span id="algo-hbos-module"></span><h2>algo.hbos module<a class="headerlink" href="#module-algo.hbos" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="algo.hbos.HBOS">
<em class="property">class </em><code class="sig-prename descclassname">algo.hbos.</code><code class="sig-name descname">HBOS</code><span class="sig-paren">(</span><em class="sig-param">n_bins=10</em>, <em class="sig-param">alpha=0.1</em>, <em class="sig-param">tol=0.5</em>, <em class="sig-param">contamination=0.1</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.hbos.HBOS" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">pyodds.algo.base.Base</span></code></p>
<p>Histogram- based outlier detection (HBOS) is an efficient unsupervised
method. It assumes the feature independence and calculates the degree
of outlyingness by building histograms. See <a href="#id13"><span class="problematic" id="id14">:cite:`goldstein2012histogram`</span></a>
for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_bins</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default=10</em><em>)</em>) – The number of bins.</p></li>
<li><p><strong>alpha</strong> (<em>float in</em><em> (</em><em>0</em><em>, </em><em>1</em><em>)</em><em>, </em><em>optional</em><em> (</em><em>default=0.1</em><em>)</em>) – The regularizer for preventing overflow.</p></li>
<li><p><strong>tol</strong> (<em>float in</em><em> (</em><em>0</em><em>, </em><em>1</em><em>)</em><em>, </em><em>optional</em><em> (</em><em>default=0.1</em><em>)</em>) – The parameter to decide the flexibility while dealing
the samples falling outside the bins.</p></li>
<li><p><strong>contamination</strong> (<em>float in</em><em> (</em><em>0.</em><em>, </em><em>0.5</em><em>)</em><em>, </em><em>optional</em><em> (</em><em>default=0.1</em><em>)</em>) – The amount of contamination of the data set,
i.e. the proportion of outliers in the data set. Used when fitting to
define the threshold on the decision function.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="algo.hbos.HBOS.bin_edges_">
<code class="sig-name descname">bin_edges_</code><a class="headerlink" href="#algo.hbos.HBOS.bin_edges_" title="Permalink to this definition">¶</a></dt>
<dd><p>The edges of the bins.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>numpy array of shape (n_bins + 1, n_features )</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="algo.hbos.HBOS.hist_">
<code class="sig-name descname">hist_</code><a class="headerlink" href="#algo.hbos.HBOS.hist_" title="Permalink to this definition">¶</a></dt>
<dd><p>The density of each histogram.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>numpy array of shape (n_bins, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="algo.hbos.HBOS.decision_scores_">
<code class="sig-name descname">decision_scores_</code><a class="headerlink" href="#algo.hbos.HBOS.decision_scores_" title="Permalink to this definition">¶</a></dt>
<dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is fitted.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>numpy array of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="algo.hbos.HBOS.threshold_">
<code class="sig-name descname">threshold_</code><a class="headerlink" href="#algo.hbos.HBOS.threshold_" title="Permalink to this definition">¶</a></dt>
<dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="algo.hbos.HBOS.labels_">
<code class="sig-name descname">labels_</code><a class="headerlink" href="#algo.hbos.HBOS.labels_" title="Permalink to this definition">¶</a></dt>
<dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int, either 0 or 1</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="algo.hbos.HBOS.decision_function">
<code class="sig-name descname">decision_function</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.hbos.HBOS.decision_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>dataframe of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>anomaly_scores</strong> – The anomaly score of the input samples.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>numpy array of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="algo.hbos.HBOS.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.hbos.HBOS.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit detector</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>dataframe of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="algo.hbos.HBOS.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.hbos.HBOS.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Return outliers with -1 and inliers with 1, with the outlierness score calculated from the <a href="#id15"><span class="problematic" id="id16">`</span></a>decision_function(X)’,
and the threshold <a href="#id17"><span class="problematic" id="id18">`</span></a>contamination’.
:param X: The input samples.
:type X: dataframe of shape (n_samples, n_features)</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>ranking</strong> – The outlierness of the input samples.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>numpy array of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="algo.hbos.invert_order">
<code class="sig-prename descclassname">algo.hbos.</code><code class="sig-name descname">invert_order</code><span class="sig-paren">(</span><em class="sig-param">scores</em>, <em class="sig-param">method='multiplication'</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.hbos.invert_order" title="Permalink to this definition">¶</a></dt>
<dd><p>Invert the order of a list of values. The smallest value becomes
the largest in the inverted list. This is useful while combining
multiple detectors since their score order could be different.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>scores</strong> (<em>list</em><em>, </em><em>array</em><em> or </em><em>numpy array with shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – The list of values to be inverted</p></li>
<li><p><strong>method</strong> (<em>str</em><em>, </em><em>optional</em><em> (</em><em>default='multiplication'</em><em>)</em>) – Methods used for order inversion. Valid methods are:
- ‘multiplication’: multiply by -1
- ‘subtraction’: max(scores) - scores</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>inverted_scores</strong> – The inverted list</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>numpy array of shape (n_samples,)</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">scores1</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">invert_order</span><span class="p">(</span><span class="n">scores1</span><span class="p">)</span>
<span class="go">array([-0.1, -0.3, -0.5, -0.7, -0.2, -0.1])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">invert_order</span><span class="p">(</span><span class="n">scores1</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;subtraction&#39;</span><span class="p">)</span>
<span class="go">array([ 0.6,  0.4,  0.2,  0. ,  0.5,  0.6])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="module-algo.iforest">
<span id="algo-iforest-module"></span><h2>algo.iforest module<a class="headerlink" href="#module-algo.iforest" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="algo.iforest.IFOREST">
<em class="property">class </em><code class="sig-prename descclassname">algo.iforest.</code><code class="sig-name descname">IFOREST</code><span class="sig-paren">(</span><em class="sig-param">n_estimators=100</em>, <em class="sig-param">max_samples='auto'</em>, <em class="sig-param">contamination='legacy'</em>, <em class="sig-param">max_features=1.0</em>, <em class="sig-param">bootstrap=False</em>, <em class="sig-param">n_jobs=None</em>, <em class="sig-param">behaviour='old'</em>, <em class="sig-param">random_state=None</em>, <em class="sig-param">verbose=0</em>, <em class="sig-param">warm_start=False</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.iforest.IFOREST" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble.iforest.IsolationForest</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">pyodds.algo.base.Base</span></code></p>
<p>Isolation Forest Algorithm
Return the anomaly score of each sample using the IsolationForest algorithm
The IsolationForest ‘isolates’ observations by randomly selecting a feature
and then randomly selecting a split value between the maximum and minimum
values of the selected feature.
Since recursive partitioning can be represented by a tree structure, the
number of splittings required to isolate a sample is equivalent to the path
length from the root node to the terminating node.
This path length, averaged over a forest of such random trees, is a
measure of normality and our decision function.
Random partitioning produces noticeably shorter paths for anomalies.
Hence, when a forest of random trees collectively produce shorter path
lengths for particular samples, they are highly likely to be anomalies.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_estimators</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default=100</em><em>)</em>) – The number of base estimators in the ensemble.</p></li>
<li><p><strong>max_samples</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>optional</em><em> (</em><em>default=&quot;auto&quot;</em><em>)</em>) – <dl class="simple">
<dt>The number of samples to draw from X to train each base estimator.</dt><dd><ul>
<li><p>If int, then draw <cite>max_samples</cite> samples.</p></li>
<li><p>If float, then draw <cite>max_samples * X.shape[0]</cite> samples.</p></li>
<li><p>If “auto”, then <cite>max_samples=min(256, n_samples)</cite>.</p></li>
</ul>
</dd>
</dl>
<p>If max_samples is larger than the number of samples provided,
all samples will be used for all trees (no sampling).</p>
</p></li>
<li><p><strong>contamination</strong> (<em>float in</em><em> (</em><em>0.</em><em>, </em><em>0.5</em><em>)</em><em>, </em><em>optional</em><em> (</em><em>default=0.1</em><em>)</em>) – <p>The amount of contamination of the data set, i.e. the proportion
of outliers in the data set. Used when fitting to define the threshold
on the decision function. If ‘auto’, the decision function threshold is
determined as in the original paper.
.. versionchanged:: 0.20</p>
<blockquote>
<div><p>The default value of <code class="docutils literal notranslate"><span class="pre">contamination</span></code> will change from 0.1 in 0.20
to <code class="docutils literal notranslate"><span class="pre">'auto'</span></code> in 0.22.</p>
</div></blockquote>
</p></li>
<li><p><strong>max_features</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>optional</em><em> (</em><em>default=1.0</em><em>)</em>) – <dl class="simple">
<dt>The number of features to draw from X to train each base estimator.</dt><dd><ul>
<li><p>If int, then draw <cite>max_features</cite> features.</p></li>
<li><p>If float, then draw <cite>max_features * X.shape[1]</cite> features.</p></li>
</ul>
</dd>
</dl>
</p></li>
<li><p><strong>bootstrap</strong> (<em>boolean</em><em>, </em><em>optional</em><em> (</em><em>default=False</em><em>)</em>) – If True, individual trees are fit on random subsets of the training
data sampled with replacement. If False, sampling without replacement
is performed.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em> or </em><em>None</em><em>, </em><em>optional</em><em> (</em><em>default=None</em><em>)</em>) – The number of jobs to run in parallel for both <cite>fit</cite> and <cite>predict</cite>.
<code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</p></li>
<li><p><strong>behaviour</strong> (<em>str</em><em>, </em><em>default='old'</em>) – <p>Behaviour of the <code class="docutils literal notranslate"><span class="pre">decision_function</span></code> which can be either ‘old’ or
‘new’. Passing <code class="docutils literal notranslate"><span class="pre">behaviour='new'</span></code> makes the <code class="docutils literal notranslate"><span class="pre">decision_function</span></code>
change to match other anomaly detection algorithm API which will be
the default behaviour in the future. As explained in details in the
<code class="docutils literal notranslate"><span class="pre">offset_</span></code> attribute documentation, the <code class="docutils literal notranslate"><span class="pre">decision_function</span></code> becomes
dependent on the contamination parameter, in such a way that 0 becomes
its natural threshold to detect outliers.
.. versionadded:: 0.20</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">behaviour</span></code> is added in 0.20 for back-compatibility purpose.</p>
</div></blockquote>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.20: </span><code class="docutils literal notranslate"><span class="pre">behaviour='old'</span></code> is deprecated in 0.20 and will not be possible
in 0.22.</p>
</div>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.22: </span><code class="docutils literal notranslate"><span class="pre">behaviour</span></code> parameter will be deprecated in 0.22 and removed in
0.24.</p>
</div>
</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>optional</em><em> (</em><em>default=None</em><em>)</em>) – If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default=0</em><em>)</em>) – Controls the verbosity of the tree building process.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>optional</em><em> (</em><em>default=False</em><em>)</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just fit a whole
new forest. See <span class="xref std std-term">the Glossary</span>.
.. versionadded:: 0.21</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="algo.iforest.IFOREST.estimators_">
<code class="sig-name descname">estimators_</code><a class="headerlink" href="#algo.iforest.IFOREST.estimators_" title="Permalink to this definition">¶</a></dt>
<dd><p>The collection of fitted sub-estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of DecisionTreeClassifier</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="algo.iforest.IFOREST.estimators_samples_">
<code class="sig-name descname">estimators_samples_</code><a class="headerlink" href="#algo.iforest.IFOREST.estimators_samples_" title="Permalink to this definition">¶</a></dt>
<dd><p>The subset of drawn samples (i.e., the in-bag samples) for each base
estimator.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of arrays</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="algo.iforest.IFOREST.max_samples_">
<code class="sig-name descname">max_samples_</code><a class="headerlink" href="#algo.iforest.IFOREST.max_samples_" title="Permalink to this definition">¶</a></dt>
<dd><p>The actual number of samples</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>integer</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="algo.iforest.IFOREST.offset_">
<code class="sig-name descname">offset_</code><a class="headerlink" href="#algo.iforest.IFOREST.offset_" title="Permalink to this definition">¶</a></dt>
<dd><p>Offset used to define the decision function from the raw scores.
We have the relation: <code class="docutils literal notranslate"><span class="pre">decision_function</span> <span class="pre">=</span> <span class="pre">score_samples</span> <span class="pre">-</span> <span class="pre">offset_</span></code>.
Assuming behaviour == ‘new’, <code class="docutils literal notranslate"><span class="pre">offset_</span></code> is defined as follows.
When the contamination parameter is set to “auto”, the offset is equal
to -0.5 as the scores of inliers are close to 0 and the scores of
outliers are close to -1. When a contamination parameter different
than “auto” is provided, the offset is defined in such a way we obtain
the expected number of outliers (samples with decision function &lt; 0)
in training.
Assuming the behaviour parameter is set to ‘old’, we always have
<code class="docutils literal notranslate"><span class="pre">offset_</span> <span class="pre">=</span> <span class="pre">-0.5</span></code>, making the decision function independent from the
contamination parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Notes</p>
<p>The implementation is based on an ensemble of ExtraTreeRegressor. The
maximum depth of each tree is set to <code class="docutils literal notranslate"><span class="pre">ceil(log_2(n))</span></code> where
<span class="math notranslate nohighlight">\(n\)</span> is the number of samples used to build the tree
(see (Liu et al., 2008) for more details).</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id19"><span class="brackets">1</span></dt>
<dd><p>Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. “Isolation forest.”
Data Mining, 2008. ICDM’08. Eighth IEEE International Conference on.</p>
</dd>
<dt class="label" id="id20"><span class="brackets">2</span></dt>
<dd><p>Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. “Isolation-based
anomaly detection.” ACM Transactions on Knowledge Discovery from
Data (TKDD) 6.1 (2012): 3.</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-algo.knn">
<span id="algo-knn-module"></span><h2>algo.knn module<a class="headerlink" href="#module-algo.knn" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="algo.knn.KNN">
<em class="property">class </em><code class="sig-prename descclassname">algo.knn.</code><code class="sig-name descname">KNN</code><span class="sig-paren">(</span><em class="sig-param">contamination=0.1</em>, <em class="sig-param">n_neighbors=5</em>, <em class="sig-param">method='largest'</em>, <em class="sig-param">radius=1.0</em>, <em class="sig-param">algorithm='auto'</em>, <em class="sig-param">leaf_size=30</em>, <em class="sig-param">metric='minkowski'</em>, <em class="sig-param">p=2</em>, <em class="sig-param">metric_params=None</em>, <em class="sig-param">n_jobs=1</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.knn.KNN" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">pyodds.algo.base.Base</span></code></p>
<p>kNN class for outlier detection.
For an observation, its distance to its kth nearest neighbor could be
viewed as the outlying score. It could be viewed as a way to measure
the density. See <a href="#id21"><span class="problematic" id="id22">:cite:`ramaswamy2000efficient,angiulli2002fast`</span></a> for
details.</p>
<p>Three kNN detectors are supported:
largest: use the distance to the kth neighbor as the outlier score
mean: use the average of all k neighbors as the outlier score
median: use the median of the distance to k neighbors as the outlier score</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>contamination</strong> (<em>float in</em><em> (</em><em>0.</em><em>, </em><em>0.5</em><em>)</em><em>, </em><em>optional</em><em> (</em><em>default=0.1</em><em>)</em>) – The amount of contamination of the data set,
i.e. the proportion of outliers in the data set. Used when fitting to
define the threshold on the decision function.</p></li>
<li><p><strong>n_neighbors</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default = 5</em><em>)</em>) – Number of neighbors to use by default for k neighbors queries.</p></li>
<li><p><strong>method</strong> (<em>str</em><em>, </em><em>optional</em><em> (</em><em>default='largest'</em><em>)</em>) – <p>{‘largest’, ‘mean’, ‘median’}</p>
<ul>
<li><p>’largest’: use the distance to the kth neighbor as the outlier score</p></li>
<li><p>’mean’: use the average of all k neighbors as the outlier score</p></li>
<li><p>’median’: use the median of the distance to k neighbors as the
outlier score</p></li>
</ul>
</p></li>
<li><p><strong>radius</strong> (<em>float</em><em>, </em><em>optional</em><em> (</em><em>default = 1.0</em><em>)</em>) – Range of parameter space to use by default for <cite>radius_neighbors</cite>
queries.</p></li>
<li><p><strong>algorithm</strong> (<em>{'auto'</em><em>, </em><em>'ball_tree'</em><em>, </em><em>'kd_tree'</em><em>, </em><em>'brute'}</em><em>, </em><em>optional</em>) – <p>Algorithm used to compute the nearest neighbors:</p>
<ul>
<li><p>’ball_tree’ will use BallTree</p></li>
<li><p>’kd_tree’ will use KDTree</p></li>
<li><p>’brute’ will use a brute-force search.</p></li>
<li><p>’auto’ will attempt to decide the most appropriate algorithm
based on the values passed to <a class="reference internal" href="#algo.knn.KNN.fit" title="algo.knn.KNN.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></a> method.</p></li>
</ul>
<p>Note: fitting on sparse input will override the setting of
this parameter, using brute force.</p>
</p></li>
<li><p><strong>leaf_size</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default = 30</em><em>)</em>) – Leaf size passed to BallTree or KDTree.  This can affect the
speed of the construction and query, as well as the memory
required to store the tree.  The optimal value depends on the
nature of the problem.</p></li>
<li><p><strong>metric</strong> (<em>string</em><em> or </em><em>callable</em><em>, </em><em>default 'minkowski'</em>) – <p>metric to use for distance computation. Any metric from scikit-learn
or scipy.spatial.distance can be used.</p>
<p>If metric is a callable function, it is called on each
pair of instances (rows) and the resulting value recorded. The callable
should take two arrays as input and return one value indicating the
distance between them. This works for Scipy’s metrics, but is less
efficient than passing the metric name as a string.</p>
<p>Distance matrices are not supported.</p>
<p>Valid values for metric are:</p>
<ul>
<li><p>from scikit-learn: [‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’,
‘manhattan’]</p></li>
<li><p>from scipy.spatial.distance: [‘braycurtis’, ‘canberra’, ‘chebyshev’,
‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’,
‘mahalanobis’, ‘matching’, ‘minkowski’, ‘rogerstanimoto’,
‘russellrao’, ‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’,
‘sqeuclidean’, ‘yule’]</p></li>
</ul>
<p>See the documentation for scipy.spatial.distance for details on these
metrics.</p>
</p></li>
<li><p><strong>p</strong> (<em>integer</em><em>, </em><em>optional</em><em> (</em><em>default = 2</em><em>)</em>) – Parameter for the Minkowski metric from
sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is
equivalent to using manhattan_distance (l1), and euclidean_distance
(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.
See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances">http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances</a></p></li>
<li><p><strong>metric_params</strong> (<em>dict</em><em>, </em><em>optional</em><em> (</em><em>default = None</em><em>)</em>) – Additional keyword arguments for the metric function.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default = 1</em><em>)</em>) – The number of parallel jobs to run for neighbors search.
If <code class="docutils literal notranslate"><span class="pre">-1</span></code>, then the number of jobs is set to the number of CPU cores.
Affects only kneighbors and kneighbors_graph methods.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="algo.knn.KNN.decision_scores_">
<code class="sig-name descname">decision_scores_</code><a class="headerlink" href="#algo.knn.KNN.decision_scores_" title="Permalink to this definition">¶</a></dt>
<dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is
fitted.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>numpy array of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="algo.knn.KNN.threshold_">
<code class="sig-name descname">threshold_</code><a class="headerlink" href="#algo.knn.KNN.threshold_" title="Permalink to this definition">¶</a></dt>
<dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="algo.knn.KNN.labels_">
<code class="sig-name descname">labels_</code><a class="headerlink" href="#algo.knn.KNN.labels_" title="Permalink to this definition">¶</a></dt>
<dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int, either 0 or 1</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="algo.knn.KNN.decision_function">
<code class="sig-name descname">decision_function</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.knn.KNN.decision_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>dataframe of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>anomaly_scores</strong> – The anomaly score of the input samples.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>numpy array of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="algo.knn.KNN.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.knn.KNN.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit detector. y is optional for unsupervised methods.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>dataframe of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="algo.knn.KNN.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.knn.KNN.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Return outliers with -1 and inliers with 1, with the outlierness score calculated from the <a href="#id23"><span class="problematic" id="id24">`</span></a>decision_function(X)’,
and the threshold <a href="#id25"><span class="problematic" id="id26">`</span></a>contamination’.
:param X: The input samples.
:type X: dataframe of shape (n_samples, n_features)</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>ranking</strong> – The outlierness of the input samples.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>numpy array of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-algo.lof">
<span id="algo-lof-module"></span><h2>algo.lof module<a class="headerlink" href="#module-algo.lof" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="algo.lof.LOF">
<em class="property">class </em><code class="sig-prename descclassname">algo.lof.</code><code class="sig-name descname">LOF</code><span class="sig-paren">(</span><em class="sig-param">n_neighbors=20</em>, <em class="sig-param">algorithm='auto'</em>, <em class="sig-param">leaf_size=30</em>, <em class="sig-param">metric='minkowski'</em>, <em class="sig-param">p=2</em>, <em class="sig-param">metric_params=None</em>, <em class="sig-param">contamination='legacy'</em>, <em class="sig-param">novelty=False</em>, <em class="sig-param">n_jobs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.lof.LOF" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.neighbors.lof.LocalOutlierFactor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">pyodds.algo.base.Base</span></code></p>
<p>Unsupervised Outlier Detection using Local Outlier Factor (LOF)
The anomaly score of each sample is called Local Outlier Factor.
It measures the local deviation of density of a given sample with
respect to its neighbors.
It is local in that the anomaly score depends on how isolated the object
is with respect to the surrounding neighborhood.
More precisely, locality is given by k-nearest neighbors, whose distance
is used to estimate the local density.
By comparing the local density of a sample to the local densities of
its neighbors, one can identify samples that have a substantially lower
density than their neighbors. These are considered outliers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_neighbors</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default=20</em><em>)</em>) – Number of neighbors to use by default for <code class="xref py py-meth docutils literal notranslate"><span class="pre">kneighbors()</span></code> queries.
If n_neighbors is larger than the number of samples provided,
all samples will be used.</p></li>
<li><p><strong>algorithm</strong> (<em>{'auto'</em><em>, </em><em>'ball_tree'</em><em>, </em><em>'kd_tree'</em><em>, </em><em>'brute'}</em><em>, </em><em>optional</em>) – <p>Algorithm used to compute the nearest neighbors:
- ‘ball_tree’ will use <code class="xref py py-class docutils literal notranslate"><span class="pre">BallTree</span></code>
- ‘kd_tree’ will use <code class="xref py py-class docutils literal notranslate"><span class="pre">KDTree</span></code>
- ‘brute’ will use a brute-force search.
- ‘auto’ will attempt to decide the most appropriate algorithm</p>
<blockquote>
<div><p>based on the values passed to <code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code> method.</p>
</div></blockquote>
<p>Note: fitting on sparse input will override the setting of
this parameter, using brute force.</p>
</p></li>
<li><p><strong>leaf_size</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default=30</em><em>)</em>) – Leaf size passed to <code class="xref py py-class docutils literal notranslate"><span class="pre">BallTree</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">KDTree</span></code>. This can
affect the speed of the construction and query, as well as the memory
required to store the tree. The optimal value depends on the
nature of the problem.</p></li>
<li><p><strong>metric</strong> (<em>string</em><em> or </em><em>callable</em><em>, </em><em>default 'minkowski'</em>) – <p>metric used for the distance computation. Any metric from scikit-learn
or scipy.spatial.distance can be used.
If ‘precomputed’, the training input X is expected to be a distance
matrix.
If metric is a callable function, it is called on each
pair of instances (rows) and the resulting value recorded. The callable
should take two arrays as input and return one value indicating the
distance between them. This works for Scipy’s metrics, but is less
efficient than passing the metric name as a string.
Valid values for metric are:</p>
<ul>
<li><p>from scikit-learn: [‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’, ‘manhattan’]</p></li>
<li><p>from scipy.spatial.distance: [‘braycurtis’, ‘canberra’, ‘chebyshev’,
‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’,
‘mahalanobis’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’,
‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’,
‘yule’]</p></li>
</ul>
<p>See the documentation for scipy.spatial.distance for details on these
metrics:
<a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/spatial.distance.html">https://docs.scipy.org/doc/scipy/reference/spatial.distance.html</a></p>
</p></li>
<li><p><strong>p</strong> (<em>integer</em><em>, </em><em>optional</em><em> (</em><em>default=2</em><em>)</em>) – Parameter for the Minkowski metric from
<code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.metrics.pairwise.pairwise_distances()</span></code>. When p = 1, this
is equivalent to using manhattan_distance (l1), and euclidean_distance
(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.</p></li>
<li><p><strong>metric_params</strong> (<em>dict</em><em>, </em><em>optional</em><em> (</em><em>default=None</em><em>)</em>) – Additional keyword arguments for the metric function.</p></li>
<li><p><strong>contamination</strong> (<em>float in</em><em> (</em><em>0.</em><em>, </em><em>0.5</em><em>)</em><em>, </em><em>optional</em><em> (</em><em>default=0.1</em><em>)</em>) – <p>The amount of contamination of the data set, i.e. the proportion
of outliers in the data set. When fitting this is used to define the
threshold on the decision function. If “auto”, the decision function
threshold is determined as in the original paper.
.. versionchanged:: 0.20</p>
<blockquote>
<div><p>The default value of <code class="docutils literal notranslate"><span class="pre">contamination</span></code> will change from 0.1 in 0.20
to <code class="docutils literal notranslate"><span class="pre">'auto'</span></code> in 0.22.</p>
</div></blockquote>
</p></li>
<li><p><strong>novelty</strong> (<em>boolean</em><em>, </em><em>default False</em>) – By default, LocalOutlierFactor is only meant to be used for outlier
detection (novelty=False). Set novelty to True if you want to use
LocalOutlierFactor for novelty detection. In this case be aware that
that you should only use predict, decision_function and score_samples
on new unseen data and not on the training set.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em> or </em><em>None</em><em>, </em><em>optional</em><em> (</em><em>default=None</em><em>)</em>) – The number of parallel jobs to run for neighbors search.
<code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.
Affects only <code class="xref py py-meth docutils literal notranslate"><span class="pre">kneighbors()</span></code> and <code class="xref py py-meth docutils literal notranslate"><span class="pre">kneighbors_graph()</span></code> methods.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="algo.lof.LOF.negative_outlier_factor_">
<code class="sig-name descname">negative_outlier_factor_</code><a class="headerlink" href="#algo.lof.LOF.negative_outlier_factor_" title="Permalink to this definition">¶</a></dt>
<dd><p>The opposite LOF of the training samples. The higher, the more normal.
Inliers tend to have a LOF score close to 1 (<code class="docutils literal notranslate"><span class="pre">negative_outlier_factor_</span></code>
close to -1), while outliers tend to have a larger LOF score.
The local outlier factor (LOF) of a sample captures its
supposed ‘degree of abnormality’.
It is the average of the ratio of the local reachability density of
a sample and those of its k-nearest neighbors.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>numpy array, shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="algo.lof.LOF.n_neighbors_">
<code class="sig-name descname">n_neighbors_</code><a class="headerlink" href="#algo.lof.LOF.n_neighbors_" title="Permalink to this definition">¶</a></dt>
<dd><p>The actual number of neighbors used for <code class="xref py py-meth docutils literal notranslate"><span class="pre">kneighbors()</span></code> queries.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>integer</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="algo.lof.LOF.offset_">
<code class="sig-name descname">offset_</code><a class="headerlink" href="#algo.lof.LOF.offset_" title="Permalink to this definition">¶</a></dt>
<dd><p>Offset used to obtain binary labels from the raw scores.
Observations having a negative_outlier_factor smaller than <cite>offset_</cite>
are detected as abnormal.
The offset is set to -1.5 (inliers score around -1), except when a
contamination parameter different than “auto” is provided. In that
case, the offset is defined in such a way we obtain the expected
number of outliers in training.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id27"><span class="brackets">1</span></dt>
<dd><p>Breunig, M. M., Kriegel, H. P., Ng, R. T., &amp; Sander, J. (2000, May).
LOF: identifying density-based local outliers. In ACM sigmod record.</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-algo.lstmad">
<span id="algo-lstmad-module"></span><h2>algo.lstmad module<a class="headerlink" href="#module-algo.lstmad" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="algo.lstmad.LSTMAD">
<em class="property">class </em><code class="sig-prename descclassname">algo.lstmad.</code><code class="sig-name descname">LSTMAD</code><span class="sig-paren">(</span><em class="sig-param">len_in=1</em>, <em class="sig-param">len_out=10</em>, <em class="sig-param">num_epochs=10</em>, <em class="sig-param">lr=0.001</em>, <em class="sig-param">batch_size=1</em>, <em class="sig-param">seed: int = None</em>, <em class="sig-param">gpu: int = None</em>, <em class="sig-param">details=True</em>, <em class="sig-param">contamination=0.05</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.lstmad.LSTMAD" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">pyodds.algo.base.Base</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">pyodds.algo.algorithm_utils.deepBase</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">pyodds.algo.algorithm_utils.PyTorchUtils</span></code></p>
<p>Malhotra, Pankaj, et al. “Long short term memory networks for anomaly detection in time series.” Proceedings. Presses universitaires de Louvain, 2015.</p>
<p>Long Short Term Memory (LSTM) networks have been
demonstrated to be particularly useful for learning sequences containing
longer term patterns of unknown length, due to their ability to maintain
long term memory. Stacking recurrent hidden layers in such networks also
enables the learning of higher level temporal features, for faster learning
with sparser representations. In this paper, we use stacked LSTM networks for anomaly/fault detection in time series. A network is trained on
non-anomalous data and used as a predictor over a number of time steps.
The resulting prediction errors are modeled as a multivariate Gaussian
distribution, which is used to assess the likelihood of anomalous behavior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>len_in</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default=1</em><em>)</em>) – The length of input layer</p></li>
<li><p><strong>len_out</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default=10</em><em>)</em>) – The length of output layer</p></li>
<li><p><strong>num_epochs</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default=100</em><em>)</em>) – The number of epochs</p></li>
<li><p><strong>lr</strong> (<em>float</em><em>, </em><em>optional</em><em> (</em><em>default=1e-3</em><em>)</em>) – The speed of learning rate</p></li>
<li><p><strong>seed</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default=None</em><em>)</em>) – The random seed</p></li>
<li><p><strong>contamination</strong> (<em>float in</em><em> (</em><em>0.</em><em>, </em><em>0.5</em><em>)</em><em>, </em><em>optional</em><em> (</em><em>default=0.05</em><em>)</em>) – The percentage of outliers</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="algo.lstmad.LSTMAD.decision_function">
<code class="sig-name descname">decision_function</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.lstmad.LSTMAD.decision_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>dataframe of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>anomaly_scores</strong> – The anomaly score of the input samples.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>numpy array of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="algo.lstmad.LSTMAD.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.lstmad.LSTMAD.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit detector.
:param X: The input samples.
:type X: dataframe of shape (n_samples, n_features)</p>
</dd></dl>

<dl class="method">
<dt id="algo.lstmad.LSTMAD.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.lstmad.LSTMAD.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Return outliers with -1 and inliers with 1, with the outlierness score calculated from the <a href="#id28"><span class="problematic" id="id29">`</span></a>decision_function(X)’,
and the threshold <a href="#id30"><span class="problematic" id="id31">`</span></a>contamination’.
:param X: The input samples.
:type X: dataframe of shape (n_samples, n_features)</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>ranking</strong> – The outlierness of the input samples.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>numpy array of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="algo.lstmad.LSTMSequence">
<em class="property">class </em><code class="sig-prename descclassname">algo.lstmad.</code><code class="sig-name descname">LSTMSequence</code><span class="sig-paren">(</span><em class="sig-param">d</em>, <em class="sig-param">batch_size: int</em>, <em class="sig-param">len_in=1</em>, <em class="sig-param">len_out=10</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.lstmad.LSTMSequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="algo.lstmad.LSTMSequence.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input_x</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.lstmad.LSTMSequence.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-algo.lstmencdec">
<span id="algo-lstmencdec-module"></span><h2>algo.lstmencdec module<a class="headerlink" href="#module-algo.lstmencdec" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="algo.lstmencdec.LSTMED">
<em class="property">class </em><code class="sig-prename descclassname">algo.lstmencdec.</code><code class="sig-name descname">LSTMED</code><span class="sig-paren">(</span><em class="sig-param">name: str = 'LSTM-ED'</em>, <em class="sig-param">num_epochs: int = 10</em>, <em class="sig-param">batch_size: int = 20</em>, <em class="sig-param">lr: float = 0.001</em>, <em class="sig-param">hidden_size: int = 5</em>, <em class="sig-param">sequence_length: int = 30</em>, <em class="sig-param">train_gaussian_percentage: float = 0.25</em>, <em class="sig-param">n_layers: tuple = (1</em>, <em class="sig-param">1)</em>, <em class="sig-param">use_bias: tuple = (True</em>, <em class="sig-param">True)</em>, <em class="sig-param">dropout: tuple = (0</em>, <em class="sig-param">0)</em>, <em class="sig-param">seed: int = None</em>, <em class="sig-param">gpu: int = None</em>, <em class="sig-param">details=True</em>, <em class="sig-param">contamination=0.05</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.lstmencdec.LSTMED" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">pyodds.algo.base.Base</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">pyodds.algo.algorithm_utils.deepBase</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">pyodds.algo.algorithm_utils.PyTorchUtils</span></code></p>
<p>Malhotra, Pankaj, et al. “LSTM-based encoder-decoder for multi-sensor anomaly detection.” ICML, 2016.</p>
<p>Mechanical devices such as engines, vehicles, aircrafts, etc., are typically instrumented with numerous sensors to capture the behavior and health of the machine. However, there are often external factors or variables which are not captured by sensors leading to time-series which are inherently unpredictable. For instance, manual controls and/or unmonitored environmental conditions or load may lead to inherently unpredictable time-series. Detecting anomalies in such scenarios becomes challenging using standard approaches based on mathematical models that rely on stationarity, or prediction models that utilize prediction errors to detect anomalies. We propose a Long Short Term Memory Networks based Encoder-Decoder scheme for Anomaly Detection (EncDec-AD) that learns to reconstruct ‘normal’ time-series behavior, and thereafter uses reconstruction error to detect anomalies. We experiment with three publicly available quasi predictable time-series datasets: power demand, space shuttle, and ECG, and two real-world engine datasets with both predictive and unpredictable behavior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>str</em><em>, </em><em>optional default='LSTM-ED'</em>) – The name of the algorithm</p></li>
<li><p><strong>num_epochs</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default=10</em><em>)</em>) – The number of epochs</p></li>
<li><p><strong>batch_size</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default=20</em><em>)</em>) – The number of batch size</p></li>
<li><p><strong>lr</strong> (<em>float</em><em>, </em><em>optional</em><em> (</em><em>default=1e-3</em><em>)</em>) – The speed of learning rate</p></li>
<li><p><strong>hidden_size</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default=5</em><em>)</em>) – The number of hidden layer</p></li>
<li><p><strong>sequence_length</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default=30</em><em>)</em>) – The length of sequence</p></li>
<li><p><strong>train_gaussian_percentage</strong> (<em>float</em><em>, </em><em>optional</em><em> (</em><em>default=0.25</em><em>)</em>) – The percentage for gaussian training</p></li>
<li><p><strong>n_layers</strong> (<em>tuple</em><em>, </em><em>optional</em><em> (</em><em>default=</em><em>(</em><em>1</em><em>,</em><em>1</em><em>)</em><em>)</em>) – The number of hidden layers</p></li>
<li><p><strong>use_bias</strong> (<em>tuple</em><em>, </em><em>optional</em><em> (</em><em>default=</em><em>(</em><em>True</em><em>, </em><em>True</em><em>)</em><em>)</em>) – Whether use bias or not in hidden layers</p></li>
<li><p><strong>dropout</strong> (<em>tuple</em><em>, </em><em>optional</em><em> (</em><em>default=</em><em>(</em><em>0</em><em>, </em><em>0</em><em>)</em><em>)</em>) – Dropout rates in hidden layers</p></li>
<li><p><strong>seed</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default=None</em><em>)</em>) – The random seed</p></li>
<li><p><strong>contamination</strong> (<em>float in</em><em> (</em><em>0.</em><em>, </em><em>0.5</em><em>)</em><em>, </em><em>optional</em><em> (</em><em>default=0.05</em><em>)</em>) – The percentage of outliers</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="algo.lstmencdec.LSTMED.decision_function">
<code class="sig-name descname">decision_function</code><span class="sig-paren">(</span><em class="sig-param">X: pandas.core.frame.DataFrame</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.lstmencdec.LSTMED.decision_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>dataframe of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>anomaly_scores</strong> – The anomaly score of the input samples.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>numpy array of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="algo.lstmencdec.LSTMED.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">X: pandas.core.frame.DataFrame</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.lstmencdec.LSTMED.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit detector.
:param X: The input samples.
:type X: dataframe of shape (n_samples, n_features)</p>
</dd></dl>

<dl class="method">
<dt id="algo.lstmencdec.LSTMED.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.lstmencdec.LSTMED.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Return outliers with -1 and inliers with 1, with the outlierness score calculated from the <a href="#id32"><span class="problematic" id="id33">`</span></a>decision_function(X)’,
and the threshold <a href="#id34"><span class="problematic" id="id35">`</span></a>contamination’.
:param X: The input samples.
:type X: dataframe of shape (n_samples, n_features)</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>ranking</strong> – The outlierness of the input samples.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>numpy array of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="algo.lstmencdec.LSTMEDModule">
<em class="property">class </em><code class="sig-prename descclassname">algo.lstmencdec.</code><code class="sig-name descname">LSTMEDModule</code><span class="sig-paren">(</span><em class="sig-param">n_features: int</em>, <em class="sig-param">hidden_size: int</em>, <em class="sig-param">n_layers: tuple</em>, <em class="sig-param">use_bias: tuple</em>, <em class="sig-param">dropout: tuple</em>, <em class="sig-param">seed: int</em>, <em class="sig-param">gpu: int</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.lstmencdec.LSTMEDModule" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">pyodds.algo.algorithm_utils.PyTorchUtils</span></code></p>
<dl class="method">
<dt id="algo.lstmencdec.LSTMEDModule.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">ts_batch</em>, <em class="sig-param">return_latent: bool = False</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.lstmencdec.LSTMEDModule.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-algo.luminolFunc">
<span id="algo-luminolfunc-module"></span><h2>algo.luminolFunc module<a class="headerlink" href="#module-algo.luminolFunc" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="algo.luminolFunc.luminolDet">
<em class="property">class </em><code class="sig-prename descclassname">algo.luminolFunc.</code><code class="sig-name descname">luminolDet</code><span class="sig-paren">(</span><em class="sig-param">contamination=0.1</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.luminolFunc.luminolDet" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">pyodds.algo.base.Base</span></code></p>
<p>Luminol is a light weight python library for time series data analysis. The two major functionalities it supports are anomaly detection and correlation. It can be used to investigate possible causes of anomaly.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>contamination</strong> (<em>float in</em><em> (</em><em>0.</em><em>, </em><em>0.5</em><em>)</em><em>, </em><em>optional</em><em> (</em><em>default=0.1</em><em>)</em>) – </p></li>
<li><p><strong>amount of contamination of the data set</strong><strong>,</strong> (<em>The</em>) – </p></li>
<li><p><strong>the proportion of outliers in the data set. Used when fitting to</strong> (<em>i.e.</em>) – </p></li>
<li><p><strong>the threshold on the decision function.</strong> (<em>define</em>) – </p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="algo.luminolFunc.luminolDet.decision_function">
<code class="sig-name descname">decision_function</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.luminolFunc.luminolDet.decision_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>dataframe of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>anomaly_scores</strong> – The anomaly score of the input samples.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>numpy array of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="algo.luminolFunc.luminolDet.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.luminolFunc.luminolDet.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit detector.
:param X: The input samples.
:type X: dataframe of shape (n_samples, n_features)</p>
</dd></dl>

<dl class="method">
<dt id="algo.luminolFunc.luminolDet.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.luminolFunc.luminolDet.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Return outliers with -1 and inliers with 1, with the outlierness score calculated from the <a href="#id36"><span class="problematic" id="id37">`</span></a>decision_function(X)’,
and the threshold <a href="#id38"><span class="problematic" id="id39">`</span></a>contamination’.
:param X: The input samples.
:type X: dataframe of shape (n_samples, n_features)</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>ranking</strong> – The outlierness of the input samples.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>numpy array of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-algo.ocsvm">
<span id="algo-ocsvm-module"></span><h2>algo.ocsvm module<a class="headerlink" href="#module-algo.ocsvm" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="algo.ocsvm.OCSVM">
<em class="property">class </em><code class="sig-prename descclassname">algo.ocsvm.</code><code class="sig-name descname">OCSVM</code><span class="sig-paren">(</span><em class="sig-param">kernel='rbf'</em>, <em class="sig-param">degree=3</em>, <em class="sig-param">gamma='auto_deprecated'</em>, <em class="sig-param">coef0=0.0</em>, <em class="sig-param">tol=0.001</em>, <em class="sig-param">nu=0.5</em>, <em class="sig-param">shrinking=True</em>, <em class="sig-param">cache_size=200</em>, <em class="sig-param">verbose=False</em>, <em class="sig-param">max_iter=-1</em>, <em class="sig-param">random_state=None</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.ocsvm.OCSVM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.svm.classes.OneClassSVM</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">pyodds.algo.base.Base</span></code></p>
<p>Unsupervised Outlier Detection.
Estimate the support of a high-dimensional distribution.
The implementation is based on libsvm.
Read more in the <span class="xref std std-ref">User Guide</span>.
:param kernel: Specifies the kernel type to be used in the algorithm.</p>
<blockquote>
<div><p>It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or
a callable.
If none is given, ‘rbf’ will be used. If a callable is given it is
used to precompute the kernel matrix.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>degree</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default=3</em><em>)</em>) – Degree of the polynomial kernel function (‘poly’).
Ignored by all other kernels.</p></li>
<li><p><strong>gamma</strong> (<em>float</em><em>, </em><em>optional</em><em> (</em><em>default='auto'</em><em>)</em>) – Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.
Current default is ‘auto’ which uses 1 / n_features,
if <code class="docutils literal notranslate"><span class="pre">gamma='scale'</span></code> is passed then it uses 1 / (n_features * X.var())
as value of gamma. The current default of gamma, ‘auto’, will change
to ‘scale’ in version 0.22. ‘auto_deprecated’, a deprecated version of
‘auto’ is used as a default indicating that no explicit value of gamma
was passed.</p></li>
<li><p><strong>coef0</strong> (<em>float</em><em>, </em><em>optional</em><em> (</em><em>default=0.0</em><em>)</em>) – Independent term in kernel function.
It is only significant in ‘poly’ and ‘sigmoid’.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>optional</em>) – Tolerance for stopping criterion.</p></li>
<li><p><strong>nu</strong> (<em>float</em><em>, </em><em>optional</em>) – An upper bound on the fraction of training
errors and a lower bound of the fraction of support
vectors. Should be in the interval (0, 1]. By default 0.5
will be taken.</p></li>
<li><p><strong>shrinking</strong> (<em>boolean</em><em>, </em><em>optional</em>) – Whether to use the shrinking heuristic.</p></li>
<li><p><strong>cache_size</strong> (<em>float</em><em>, </em><em>optional</em>) – Specify the size of the kernel cache (in MB).</p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em>, </em><em>default: False</em>) – Enable verbose output. Note that this setting takes advantage of a
per-process runtime setting in libsvm that, if enabled, may not work
properly in a multithreaded context.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default=-1</em><em>)</em>) – Hard limit on iterations within solver, or -1 for no limit.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>optional</em><em> (</em><em>default=None</em><em>)</em>) – <p>Ignored.
.. deprecated:: 0.20</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">random_state</span></code> has been deprecated in 0.20 and will be removed in
0.22.</p>
</div></blockquote>
</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="algo.ocsvm.OCSVM.support_">
<code class="sig-name descname">support_</code><a class="headerlink" href="#algo.ocsvm.OCSVM.support_" title="Permalink to this definition">¶</a></dt>
<dd><p>Indices of support vectors.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like, shape = [n_SV]</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="algo.ocsvm.OCSVM.support_vectors_">
<code class="sig-name descname">support_vectors_</code><a class="headerlink" href="#algo.ocsvm.OCSVM.support_vectors_" title="Permalink to this definition">¶</a></dt>
<dd><p>Support vectors.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like, shape = [nSV, n_features]</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="algo.ocsvm.OCSVM.dual_coef_">
<code class="sig-name descname">dual_coef_</code><a class="headerlink" href="#algo.ocsvm.OCSVM.dual_coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Coefficients of the support vectors in the decision function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array, shape = [1, n_SV]</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="algo.ocsvm.OCSVM.coef_">
<code class="sig-name descname">coef_</code><a class="headerlink" href="#algo.ocsvm.OCSVM.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Weights assigned to the features (coefficients in the primal
problem). This is only available in the case of a linear kernel.
<cite>coef_</cite> is readonly property derived from <cite>dual_coef_</cite> and
<cite>support_vectors_</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array, shape = [1, n_features]</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="algo.ocsvm.OCSVM.intercept_">
<code class="sig-name descname">intercept_</code><a class="headerlink" href="#algo.ocsvm.OCSVM.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Constant in the decision function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array, shape = [1,]</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="algo.ocsvm.OCSVM.offset_">
<code class="sig-name descname">offset_</code><a class="headerlink" href="#algo.ocsvm.OCSVM.offset_" title="Permalink to this definition">¶</a></dt>
<dd><p>Offset used to define the decision function from the raw scores.
We have the relation: decision_function = score_samples - <cite>offset_</cite>.
The offset is the opposite of <cite>intercept_</cite> and is provided for
consistency with other outlier detection algorithms.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="k">import</span> <span class="n">OneClassSVM</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.44</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.45</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.46</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">OneClassSVM</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([-1,  1,  1,  1, -1])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score_samples</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  
<span class="go">array([1.7798..., 2.0547..., 2.0556..., 2.0561..., 1.7332...])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="module-algo.pca">
<span id="algo-pca-module"></span><h2>algo.pca module<a class="headerlink" href="#module-algo.pca" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="algo.pca.PCA">
<em class="property">class </em><code class="sig-prename descclassname">algo.pca.</code><code class="sig-name descname">PCA</code><span class="sig-paren">(</span><em class="sig-param">n_components=None</em>, <em class="sig-param">n_selected_components=None</em>, <em class="sig-param">contamination=0.1</em>, <em class="sig-param">copy=True</em>, <em class="sig-param">whiten=False</em>, <em class="sig-param">svd_solver='auto'</em>, <em class="sig-param">tol=0.0</em>, <em class="sig-param">iterated_power='auto'</em>, <em class="sig-param">random_state=None</em>, <em class="sig-param">weighted=True</em>, <em class="sig-param">standardization=True</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.pca.PCA" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">pyodds.algo.base.Base</span></code></p>
<p>Principal component analysis (PCA) can be used in detecting outliers. PCA
is a linear dimensionality reduction using Singular Value Decomposition
of the data to project it to a lower dimensional space.</p>
<p>In this procedure, covariance matrix of the data can be decomposed to
orthogonal vectors, called eigenvectors, associated with eigenvalues. The
eigenvectors with high eigenvalues capture most of the variance in the
data.</p>
<p>Therefore, a low dimensional hyperplane constructed by k eigenvectors can
capture most of the variance in the data. However, outliers are different
from normal data points, which is more obvious on the hyperplane
constructed by the eigenvectors with small eigenvalues.</p>
<p>Therefore, outlier scores can be obtained as the sum of the projected
distance of a sample on all eigenvectors.
See <a href="#id40"><span class="problematic" id="id41">:cite:`shyu2003novel,aggarwal2015outlier`</span></a> for details.</p>
<p>Score(X) = Sum of weighted euclidean distance between each sample to the
hyperplane constructed by the selected eigenvectors</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_components</strong> (<em>int</em><em>, </em><em>float</em><em>, </em><em>None</em><em> or </em><em>string</em>) – <p>Number of components to keep.
if n_components is not set all components are kept:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">n_components</span> <span class="o">==</span> <span class="nb">min</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
</pre></div>
</div>
<p>if n_components == ‘mle’ and svd_solver == ‘full’, Minka’s MLE is used
to guess the dimension
if <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;</span> <span class="pre">n_components</span> <span class="pre">&lt;</span> <span class="pre">1</span></code> and svd_solver == ‘full’, select the number
of components such that the amount of variance that needs to be
explained is greater than the percentage specified by n_components
n_components cannot be equal to n_features for svd_solver == ‘arpack’.</p>
</p></li>
<li><p><strong>n_selected_components</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default=None</em><em>)</em>) – Number of selected principal components
for calculating the outlier scores. It is not necessarily equal to
the total number of the principal components. If not set, use
all principal components.</p></li>
<li><p><strong>contamination</strong> (<em>float in</em><em> (</em><em>0.</em><em>, </em><em>0.5</em><em>)</em><em>, </em><em>optional</em><em> (</em><em>default=0.1</em><em>)</em>) – The amount of contamination of the data set, i.e.
the proportion of outliers in the data set. Used when fitting to
define the threshold on the decision function.</p></li>
<li><p><strong>copy</strong> (<em>bool</em><em> (</em><em>default True</em><em>)</em>) – If False, data passed to fit are overwritten and running
fit(X).transform(X) will not yield the expected results,
use fit_transform(X) instead.</p></li>
<li><p><strong>whiten</strong> (<em>bool</em><em>, </em><em>optional</em><em> (</em><em>default False</em><em>)</em>) – <p>When True (False by default) the <cite>components_</cite> vectors are multiplied
by the square root of n_samples and then divided by the singular values
to ensure uncorrelated outputs with unit component-wise variances.</p>
<p>Whitening will remove some information from the transformed signal
(the relative variance scales of the components) but can sometime
improve the predictive accuracy of the downstream estimators by
making their data respect some hard-wired assumptions.</p>
</p></li>
<li><p><strong>svd_solver</strong> (<em>string {'auto'</em><em>, </em><em>'full'</em><em>, </em><em>'arpack'</em><em>, </em><em>'randomized'}</em>) – <dl class="simple">
<dt>auto :</dt><dd><p>the solver is selected by a default policy based on <cite>X.shape</cite> and
<cite>n_components</cite>: if the input data is larger than 500x500 and the
number of components to extract is lower than 80% of the smallest
dimension of the data, then the more efficient ‘randomized’
method is enabled. Otherwise the exact full SVD is computed and
optionally truncated afterwards.</p>
</dd>
<dt>full :</dt><dd><p>run exact full SVD calling the standard LAPACK solver via
<cite>scipy.linalg.svd</cite> and select the components by postprocessing</p>
</dd>
<dt>arpack :</dt><dd><p>run SVD truncated to n_components calling ARPACK solver via
<cite>scipy.sparse.linalg.svds</cite>. It requires strictly
0 &lt; n_components &lt; X.shape[1]</p>
</dd>
<dt>randomized :</dt><dd><p>run randomized SVD by the method of Halko et al.</p>
</dd>
</dl>
</p></li>
<li><p><strong>tol</strong> (<em>float &gt;= 0</em><em>, </em><em>optional</em><em> (</em><em>default .0</em><em>)</em>) – Tolerance for singular values computed by svd_solver == ‘arpack’.</p></li>
<li><p><strong>iterated_power</strong> (<em>int &gt;= 0</em><em>, or </em><em>'auto'</em><em>, </em><em>(</em><em>default 'auto'</em><em>)</em>) – Number of iterations for the power method computed by
svd_solver == ‘randomized’.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>optional</em><em> (</em><em>default None</em><em>)</em>) – If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>. Used when <code class="docutils literal notranslate"><span class="pre">svd_solver</span></code> == ‘arpack’ or ‘randomized’.</p></li>
<li><p><strong>weighted</strong> (<em>bool</em><em>, </em><em>optional</em><em> (</em><em>default=True</em><em>)</em>) – If True, the eigenvalues are used in score computation.
The eigenvectors with small eigenvalues comes with more importance
in outlier score calculation.</p></li>
<li><p><strong>standardization</strong> (<em>bool</em><em>, </em><em>optional</em><em> (</em><em>default=True</em><em>)</em>) – If True, perform standardization first to convert
data to zero mean and unit variance.
See <a class="reference external" href="http://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html">http://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html</a></p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="algo.pca.PCA.components_">
<code class="sig-name descname">components_</code><a class="headerlink" href="#algo.pca.PCA.components_" title="Permalink to this definition">¶</a></dt>
<dd><p>Principal axes in feature space, representing the directions of
maximum variance in the data. The components are sorted by
<code class="docutils literal notranslate"><span class="pre">explained_variance_</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array, shape (n_components, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="algo.pca.PCA.explained_variance_">
<code class="sig-name descname">explained_variance_</code><a class="headerlink" href="#algo.pca.PCA.explained_variance_" title="Permalink to this definition">¶</a></dt>
<dd><p>The amount of variance explained by each of the selected components.</p>
<p>Equal to n_components largest eigenvalues
of the covariance matrix of X.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array, shape (n_components,)</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="algo.pca.PCA.explained_variance_ratio_">
<code class="sig-name descname">explained_variance_ratio_</code><a class="headerlink" href="#algo.pca.PCA.explained_variance_ratio_" title="Permalink to this definition">¶</a></dt>
<dd><p>Percentage of variance explained by each of the selected components.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">n_components</span></code> is not set then all components are stored and the
sum of explained variances is equal to 1.0.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array, shape (n_components,)</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="algo.pca.PCA.singular_values_">
<code class="sig-name descname">singular_values_</code><a class="headerlink" href="#algo.pca.PCA.singular_values_" title="Permalink to this definition">¶</a></dt>
<dd><p>The singular values corresponding to each of the selected components.
The singular values are equal to the 2-norms of the <code class="docutils literal notranslate"><span class="pre">n_components</span></code>
variables in the lower-dimensional space.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array, shape (n_components,)</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="algo.pca.PCA.mean_">
<code class="sig-name descname">mean_</code><a class="headerlink" href="#algo.pca.PCA.mean_" title="Permalink to this definition">¶</a></dt>
<dd><p>Per-feature empirical mean, estimated from the training set.</p>
<p>Equal to <cite>X.mean(axis=0)</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array, shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="algo.pca.PCA.n_components_">
<code class="sig-name descname">n_components_</code><a class="headerlink" href="#algo.pca.PCA.n_components_" title="Permalink to this definition">¶</a></dt>
<dd><p>The estimated number of components. When n_components is set
to ‘mle’ or a number between 0 and 1 (with svd_solver == ‘full’) this
number is estimated from input data. Otherwise it equals the parameter
n_components, or n_features if n_components is None.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="algo.pca.PCA.noise_variance_">
<code class="sig-name descname">noise_variance_</code><a class="headerlink" href="#algo.pca.PCA.noise_variance_" title="Permalink to this definition">¶</a></dt>
<dd><p>The estimated noise covariance following the Probabilistic PCA model
from Tipping and Bishop 1999. See “Pattern Recognition and
Machine Learning” by C. Bishop, 12.2.1 p. 574 or
<a class="reference external" href="http://www.miketipping.com/papers/met-mppca.pdf">http://www.miketipping.com/papers/met-mppca.pdf</a>. It is required to
computed the estimated data covariance and score samples.</p>
<p>Equal to the average of (min(n_features, n_samples) - n_components)
smallest eigenvalues of the covariance matrix of X.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="algo.pca.PCA.decision_scores_">
<code class="sig-name descname">decision_scores_</code><a class="headerlink" href="#algo.pca.PCA.decision_scores_" title="Permalink to this definition">¶</a></dt>
<dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is fitted.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>numpy array of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="algo.pca.PCA.threshold_">
<code class="sig-name descname">threshold_</code><a class="headerlink" href="#algo.pca.PCA.threshold_" title="Permalink to this definition">¶</a></dt>
<dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="algo.pca.PCA.labels_">
<code class="sig-name descname">labels_</code><a class="headerlink" href="#algo.pca.PCA.labels_" title="Permalink to this definition">¶</a></dt>
<dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int, either 0 or 1</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="algo.pca.PCA.decision_function">
<code class="sig-name descname">decision_function</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.pca.PCA.decision_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>dataframe of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>anomaly_scores</strong> – The anomaly score of the input samples.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>numpy array of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">explained_variance_</code></dt>
<dd><p>The amount of variance explained by each of the selected components.</p>
<p>Equal to n_components largest eigenvalues
of the covariance matrix of X.</p>
<p>Decorator for scikit-learn PCA attributes.</p>
</dd></dl>

<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">explained_variance_ratio_</code></dt>
<dd><p>Percentage of variance explained by each of the selected components.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">n_components</span></code> is not set then all components are stored and the
sum of explained variances is equal to 1.0.</p>
<p>Decorator for scikit-learn PCA attributes.</p>
</dd></dl>

<dl class="method">
<dt id="algo.pca.PCA.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.pca.PCA.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit detector.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>dataframe of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">mean_</code></dt>
<dd><p>Per-feature empirical mean, estimated from the training set.</p>
<p>Decorator for scikit-learn PCA attributes.</p>
</dd></dl>

<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">noise_variance_</code></dt>
<dd><p>The estimated noise covariance following the Probabilistic PCA model
from Tipping and Bishop 1999. See “Pattern Recognition and
Machine Learning” by C. Bishop, 12.2.1 p. 574 or
<a class="reference external" href="http://www.miketipping.com/papers/met-mppca.pdf">http://www.miketipping.com/papers/met-mppca.pdf</a>. It is required to
computed the estimated data covariance and score samples.</p>
<p>Equal to the average of (min(n_features, n_samples) - n_components)
smallest eigenvalues of the covariance matrix of X.</p>
<p>Decorator for scikit-learn PCA attributes.</p>
</dd></dl>

<dl class="method">
<dt id="algo.pca.PCA.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.pca.PCA.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Return outliers with -1 and inliers with 1, with the outlierness score calculated from the <a href="#id42"><span class="problematic" id="id43">`</span></a>decision_function(X)’,
and the threshold <a href="#id44"><span class="problematic" id="id45">`</span></a>contamination’.
:param X: The input samples.
:type X: dataframe of shape (n_samples, n_features)</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>ranking</strong> – The outlierness of the input samples.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>numpy array of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">singular_values_</code></dt>
<dd><p>The singular values corresponding to each of the selected
components. The singular values are equal to the 2-norms of the
<code class="docutils literal notranslate"><span class="pre">n_components</span></code> variables in the lower-dimensional space.</p>
<p>Decorator for scikit-learn PCA attributes.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-algo.robustcovariance">
<span id="algo-robustcovariance-module"></span><h2>algo.robustcovariance module<a class="headerlink" href="#module-algo.robustcovariance" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="algo.robustcovariance.RCOV">
<em class="property">class </em><code class="sig-prename descclassname">algo.robustcovariance.</code><code class="sig-name descname">RCOV</code><span class="sig-paren">(</span><em class="sig-param">store_precision=True</em>, <em class="sig-param">assume_centered=False</em>, <em class="sig-param">support_fraction=None</em>, <em class="sig-param">contamination=0.1</em>, <em class="sig-param">random_state=None</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.robustcovariance.RCOV" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.covariance.elliptic_envelope.EllipticEnvelope</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">pyodds.algo.base.Base</span></code></p>
<blockquote>
<div><p>An object for detecting outliers in a Gaussian distributed dataset.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>store_precision</strong> (<em>boolean</em><em>, </em><em>optional</em><em> (</em><em>default=True</em><em>)</em>) – Specify if the estimated precision is stored.</p></li>
<li><p><strong>assume_centered</strong> (<em>boolean</em><em>, </em><em>optional</em><em> (</em><em>default=False</em><em>)</em>) – If True, the support of robust location and covariance estimates
is computed, and a covariance estimate is recomputed from it,
without centering the data.
Useful to work with data whose mean is significantly equal to
zero but is not exactly zero.
If False, the robust location and covariance are directly computed
with the FastMCD algorithm without additional treatment.</p></li>
<li><p><strong>support_fraction</strong> (<em>float in</em><em> (</em><em>0.</em><em>, </em><em>1.</em><em>)</em><em>, </em><em>optional</em><em> (</em><em>default=None</em><em>)</em>) – The proportion of points to be included in the support of the raw
MCD estimate. If None, the minimum value of support_fraction will
be used within the algorithm: <cite>[n_sample + n_features + 1] / 2</cite>.</p></li>
<li><p><strong>contamination</strong> (<em>float in</em><em> (</em><em>0.</em><em>, </em><em>0.5</em><em>)</em><em>, </em><em>optional</em><em> (</em><em>default=0.1</em><em>)</em>) – The amount of contamination of the data set, i.e. the proportion
of outliers in the data set.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>optional</em><em> (</em><em>default=None</em><em>)</em>) – The seed of the pseudo random number generator to use when shuffling
the data.  If int, random_state is the seed used by the random number
generator; If RandomState instance, random_state is the random number
generator; If None, the random number generator is the RandomState
instance used by <cite>np.random</cite>.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="algo.robustcovariance.RCOV.location_">
<code class="sig-name descname">location_</code><a class="headerlink" href="#algo.robustcovariance.RCOV.location_" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimated robust location</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like, shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="algo.robustcovariance.RCOV.covariance_">
<code class="sig-name descname">covariance_</code><a class="headerlink" href="#algo.robustcovariance.RCOV.covariance_" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimated robust covariance matrix</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like, shape (n_features, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="algo.robustcovariance.RCOV.precision_">
<code class="sig-name descname">precision_</code><a class="headerlink" href="#algo.robustcovariance.RCOV.precision_" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimated pseudo inverse matrix.
(stored only if store_precision is True)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like, shape (n_features, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="algo.robustcovariance.RCOV.support_">
<code class="sig-name descname">support_</code><a class="headerlink" href="#algo.robustcovariance.RCOV.support_" title="Permalink to this definition">¶</a></dt>
<dd><p>A mask of the observations that have been used to compute the
robust estimates of location and shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like, shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="algo.robustcovariance.RCOV.offset_">
<code class="sig-name descname">offset_</code><a class="headerlink" href="#algo.robustcovariance.RCOV.offset_" title="Permalink to this definition">¶</a></dt>
<dd><p>Offset used to define the decision function from the raw scores.
We have the relation: <code class="docutils literal notranslate"><span class="pre">decision_function</span> <span class="pre">=</span> <span class="pre">score_samples</span> <span class="pre">-</span> <span class="pre">offset_</span></code>.
The offset depends on the contamination parameter and is defined in
such a way we obtain the expected number of outliers (samples with
decision function &lt; 0) in training.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.covariance</span> <span class="k">import</span> <span class="n">EllipticEnvelope</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">true_cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="o">.</span><span class="mi">3</span><span class="p">],</span>
<span class="gp">... </span>                     <span class="p">[</span><span class="o">.</span><span class="mi">3</span><span class="p">,</span> <span class="o">.</span><span class="mi">4</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="gp">... </span>                                                 <span class="n">cov</span><span class="o">=</span><span class="n">true_cov</span><span class="p">,</span>
<span class="gp">... </span>                                                 <span class="n">size</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cov</span> <span class="o">=</span> <span class="n">EllipticEnvelope</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># predict returns 1 for an inlier and -1 for an outlier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cov</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="gp">... </span>             <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<span class="go">array([ 1, -1])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cov</span><span class="o">.</span><span class="n">covariance_</span> 
<span class="go">array([[0.7411..., 0.2535...],</span>
<span class="go">       [0.2535..., 0.3053...]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cov</span><span class="o">.</span><span class="n">location_</span>
<span class="go">array([0.0813... , 0.0427...])</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><code class="xref py py-class docutils literal notranslate"><span class="pre">EmpiricalCovariance</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">MinCovDet</span></code></p>
</div>
<p class="rubric">Notes</p>
<p>Outlier detection from covariance estimation may break or not
perform well in high-dimensional settings. In particular, one will
always take care to work with <code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">&gt;</span> <span class="pre">n_features</span> <span class="pre">**</span> <span class="pre">2</span></code>.</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id46"><span class="brackets">1</span></dt>
<dd><p>Rousseeuw, P.J., Van Driessen, K. “A fast algorithm for the
minimum covariance determinant estimator” Technometrics 41(3), 212
(1999)</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-algo.sod">
<span id="algo-sod-module"></span><h2>algo.sod module<a class="headerlink" href="#module-algo.sod" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="algo.sod.SOD">
<em class="property">class </em><code class="sig-prename descclassname">algo.sod.</code><code class="sig-name descname">SOD</code><span class="sig-paren">(</span><em class="sig-param">contamination=0.1</em>, <em class="sig-param">n_neighbors=20</em>, <em class="sig-param">ref_set=10</em>, <em class="sig-param">alpha=0.8</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.sod.SOD" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">pyodds.algo.base.Base</span></code></p>
<p>Subspace outlier detection (SOD) schema aims to detect outlier in
varying subspaces of a high dimensional feature space. For each data
object, SOD explores the axis-parallel subspace spanned by the data
object’s neighbors and determines how much the object deviates from the
neighbors in this subspace.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_neighbors</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default=20</em><em>)</em>) – Number of neighbors to use by default for k neighbors queries.</p></li>
<li><p><strong>ref_set</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default=10</em><em>)</em>) – Specifies the number of shared nearest neighbors to create the reference set. Note that ref_set must be smaller than n_neighbors.</p></li>
<li><p><strong>alpha</strong> (<em>float in</em><em> (</em><em>0.</em><em>, </em><em>1.</em><em>)</em><em>, </em><em>optional</em><em> (</em><em>default=0.8</em><em>)</em>) – specifies the lower limit for selecting subspace.
0.8 is set as default as suggested in the original paper.</p></li>
<li><p><strong>contamination</strong> (<em>float in</em><em> (</em><em>0.</em><em>, </em><em>0.5</em><em>)</em><em>, </em><em>optional</em><em> (</em><em>default=0.1</em><em>)</em>) – The amount of contamination of the data set, i.e.
the proportion of outliers in the data set. Used when fitting to
define the threshold on the decision function.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="algo.sod.SOD.decision_scores_">
<code class="sig-name descname">decision_scores_</code><a class="headerlink" href="#algo.sod.SOD.decision_scores_" title="Permalink to this definition">¶</a></dt>
<dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is
fitted.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>numpy array of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="algo.sod.SOD.threshold_">
<code class="sig-name descname">threshold_</code><a class="headerlink" href="#algo.sod.SOD.threshold_" title="Permalink to this definition">¶</a></dt>
<dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="algo.sod.SOD.labels_">
<code class="sig-name descname">labels_</code><a class="headerlink" href="#algo.sod.SOD.labels_" title="Permalink to this definition">¶</a></dt>
<dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int, either 0 or 1</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="algo.sod.SOD.decision_function">
<code class="sig-name descname">decision_function</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.sod.SOD.decision_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>dataframe of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>anomaly_scores</strong> – The anomaly score of the input samples.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>numpy array of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="algo.sod.SOD.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.sod.SOD.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit detector.
:param X: The input samples.
:type X: dataframe of shape (n_samples, n_features)</p>
</dd></dl>

<dl class="method">
<dt id="algo.sod.SOD.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.sod.SOD.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Return outliers with -1 and inliers with 1, with the outlierness score calculated from the <a href="#id47"><span class="problematic" id="id48">`</span></a>decision_function(X)’,
and the threshold <a href="#id49"><span class="problematic" id="id50">`</span></a>contamination’.
:param X: The input samples.
:type X: dataframe of shape (n_samples, n_features)</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>ranking</strong> – The outlierness of the input samples.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>numpy array of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-algo.staticautoencoder">
<span id="algo-staticautoencoder-module"></span><h2>algo.staticautoencoder module<a class="headerlink" href="#module-algo.staticautoencoder" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="algo.staticautoencoder.StaticAutoEncoder">
<em class="property">class </em><code class="sig-prename descclassname">algo.staticautoencoder.</code><code class="sig-name descname">StaticAutoEncoder</code><span class="sig-paren">(</span><em class="sig-param">hidden_neurons=None</em>, <em class="sig-param">epoch=100</em>, <em class="sig-param">dropout_rate=0.2</em>, <em class="sig-param">contamination=0.1</em>, <em class="sig-param">regularizer_weight=0.1</em>, <em class="sig-param">activation='relu'</em>, <em class="sig-param">kernel_regularizer=0.01</em>, <em class="sig-param">loss_function='mse'</em>, <em class="sig-param">optimizer='adam'</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.staticautoencoder.StaticAutoEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">pyodds.algo.base.Base</span></code></p>
<dl class="method">
<dt id="algo.staticautoencoder.StaticAutoEncoder.decision_function">
<code class="sig-name descname">decision_function</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.staticautoencoder.StaticAutoEncoder.decision_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>dataframe of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>anomaly_scores</strong> – The anomaly score of the input samples.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>numpy array of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="algo.staticautoencoder.StaticAutoEncoder.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.staticautoencoder.StaticAutoEncoder.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit detector.
:param X: The input samples.
:type X: dataframe of shape (n_samples, n_features)</p>
</dd></dl>

<dl class="method">
<dt id="algo.staticautoencoder.StaticAutoEncoder.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.staticautoencoder.StaticAutoEncoder.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Return outliers with -1 and inliers with 1, with the outlierness score calculated from the <a href="#id51"><span class="problematic" id="id52">`</span></a>decision_function(X)’,
and the threshold <a href="#id53"><span class="problematic" id="id54">`</span></a>contamination’.
:param X: The input samples.
:type X: dataframe of shape (n_samples, n_features)</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>ranking</strong> – The outlierness of the input samples.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>numpy array of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="algo.staticautoencoder.l21shrink">
<code class="sig-prename descclassname">algo.staticautoencoder.</code><code class="sig-name descname">l21shrink</code><span class="sig-paren">(</span><em class="sig-param">epsilon</em>, <em class="sig-param">x</em><span class="sig-paren">)</span><a class="headerlink" href="#algo.staticautoencoder.l21shrink" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-algo">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-algo" title="Permalink to this headline">¶</a></h2>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="utils.html" class="btn btn-neutral float-right" title="utils package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="modules.html" class="btn btn-neutral float-left" title="pyodds" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Yuening Li, Daochen Zha, Na Zou, Xia Hu

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>
